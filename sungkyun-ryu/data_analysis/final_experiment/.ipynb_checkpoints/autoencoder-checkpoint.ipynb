{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddcf441-ae45-4e36-a3ff-bc484478a44e",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39537e03-7b43-4d1b-8992-6e6bf166543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdm_functions as fns\n",
    "import torch \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c005a8-b04f-4b65-8edb-68ef6bd4be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_normal = pd.read_csv('../dataset/5528_drop_imbalance_normal.csv')\n",
    "f_error = pd.read_csv('../dataset/5528_drop_imbalance_error.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a516a8b-82fb-4d77-9e4b-80c5684617d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_normal['created_at'] = pd.to_datetime(f_normal['created_at'], unit='s')\n",
    "f_normal = f_normal.sort_values(by='created_at')\n",
    " \n",
    "f_error['created_at'] = pd.to_datetime(f_error['created_at'], unit='s')\n",
    "f_error = f_error.sort_values(by='created_at')\n",
    "\n",
    "f_normal = f_normal.drop(columns=['asset_id', 'created_at', 'created_at_datetime', 'looseness_health', 'time','misalignment_health', 'bearing_health', 'imbalance_health'])\n",
    "f_error = f_error.drop(columns=['asset_id', 'created_at', 'created_at_datetime', 'looseness_health', 'time','misalignment_health', 'bearing_health', 'imbalance_health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3ffe5b-28a2-431b-997f-3f3f51c293f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2220, 38]), torch.Size([192, 38]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_tensor = torch.tensor(f_normal.values, dtype=torch.float32)\n",
    "f_error_tensor = torch.tensor(f_error.values, dtype=torch.float32)\n",
    "\n",
    "f_normal_tensor.shape, f_error_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe7fde9-5a27-450a-a11c-3604c9c248c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1020, 38]), torch.Size([1200, 38]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_test = f_normal_tensor[:1200]\n",
    "f_normal_train = f_normal_tensor[1200:]\n",
    "\n",
    "f_normal_train.shape, f_normal_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9fd58d-9c95-453b-b664-8b91124b26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_normal_label = torch.ones((1200))\n",
    "f_error_label = torch.zeros((192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d8c249-f1ac-45b1-b64d-fc8d3dceaab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0006ae-fee5-4a17-8979-4fd77361cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = torch.concat((f_normal_test, f_error_tensor), dim = 0) \n",
    "f_test_label = torch.concat((f_normal_label, f_error_label), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1735cd54-e097-4737-b555-d51043cb2575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1392, 38]), torch.Size([1392]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_test.shape, f_test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea70d05-0249-4306-96e2-2835e5bc2632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_test_label[f_test_label != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b71003-d55f-4412-881b-1a54113030cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1020, 38])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b80883b-d75d-4429-afc5-b2c4851fb037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 342.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1105, 1, 12, 38])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_train = f_normal_train.reshape((-1, 1, 12, 38))\n",
    "f_normal_train = fns.multi_datasets_stacks(f_normal_train, multi_dim = 13, num_groups = 12 )\n",
    "f_normal_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cdd1aa-fc0e-4e20-aeeb-505fcdb3a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13260, 38])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_normal_train = f_normal_train.reshape(-1, 38)\n",
    "f_normal_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9c1843e-9e25-4004-a7d5-37489f4a03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'train' : f_normal_train, 'test' : f_test, 'test_label': f_test_label}, 'datasets/auto_encoder_supervised.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d472c7-00ad-402b-8df2-e1a5563ea0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9052\\2151635847.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('datasets/auto_encoder_supervised.pt')\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('datasets/auto_encoder_supervised.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e739840-383f-4364-9372-b9895374f02c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data = data['train']\n",
    "test_data =data['test']\n",
    "test_label = data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fbef96-baf5-4e0b-92d1-14f3c6ab188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d76242-b3bd-4772-a5d1-101ba961e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data, dtype = torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718b3ef8-477d-438b-9575-ae96425f7c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1105, 456]), torch.Size([116, 456]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reshape((-1, 456))\n",
    "test_data = test_data.reshape((-1, 456))\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9eb4fc7-84c9-41c4-9285-131d53f208b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = test_label.reshape((-1,12))[:,0]\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4571470-d7db-46b0-a094-9172d1d77d0e",
   "metadata": {},
   "source": [
    "## <span style='color:white'> ================================================================================================================================================================= </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635989fd-cd74-4cbd-ae61-9484e01d877b",
   "metadata": {},
   "source": [
    "## Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57bd94e9-074a-410f-817c-b047666eccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AnomalyDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(456, 228),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(228, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 228),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(228, 456),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize model\n",
    "autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b28667-a9c6-4289-b424-b9effb5fd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()  # MAE in PyTorch\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d576993-5955-40e8-9ed0-7d0bbba25f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_data)\n",
    "test_dataset = TensorDataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebfd2a2f-a21e-493a-93a8-24b78bbd4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_dataset, batch_size = 512, shuffle = True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size = 512, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f15708d-66da-4882-a4ba-0313e3f576d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.32682206233342487\n",
      "Epoch [2/20], Loss: 0.3228279451529185\n",
      "Epoch [3/20], Loss: 0.31604917844136554\n",
      "Epoch [4/20], Loss: 0.3058888912200928\n",
      "Epoch [5/20], Loss: 0.28914694984753925\n",
      "Epoch [6/20], Loss: 0.26719648639361065\n",
      "Epoch [7/20], Loss: 0.2352629005908966\n",
      "Epoch [8/20], Loss: 0.1932392716407776\n",
      "Epoch [9/20], Loss: 0.149220272898674\n",
      "Epoch [10/20], Loss: 0.12060920894145966\n",
      "Epoch [11/20], Loss: 0.11775658776362737\n",
      "Epoch [12/20], Loss: 0.12220300237337749\n",
      "Epoch [13/20], Loss: 0.1216493546962738\n",
      "Epoch [14/20], Loss: 0.11789658665657043\n",
      "Epoch [15/20], Loss: 0.11205101509888966\n",
      "Epoch [16/20], Loss: 0.10610700647036235\n",
      "Epoch [17/20], Loss: 0.10250098258256912\n",
      "Epoch [18/20], Loss: 0.10433249175548553\n",
      "Epoch [19/20], Loss: 0.10323866705099742\n",
      "Epoch [20/20], Loss: 0.10107635706663132\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for x_batch in dataloader_train:\n",
    "        # inputs = x_batch\n",
    "        outputs = autoencoder(x_batch[0])\n",
    "        loss = criterion(outputs, x_batch[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c22145f4-4edd-498d-b702-27d983e82731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab59c22b-1a30-42ac-b535-b9bd6fc042ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0204, 0.0512, 0.0191, 0.0124, 0.0216, 0.0261, 0.0114, 0.0349, 0.0200,\n",
      "        0.0492, 0.0308, 0.0304, 0.0709, 0.0337, 0.0139, 0.0375, 0.0409, 0.0530,\n",
      "        0.0693, 0.0365, 0.0528, 0.0146, 0.0210, 0.0346, 0.0685, 0.0368, 0.0688,\n",
      "        0.0306, 0.0292, 0.0436, 0.0676, 0.0278, 0.0319, 0.0346, 0.0370, 0.0294,\n",
      "        0.0690, 0.0311, 0.0217, 0.0574, 0.0298, 0.0609, 0.0148, 0.0539, 0.0474,\n",
      "        0.0367, 0.0361, 0.0274, 0.0500, 0.0229, 0.0285, 0.0225, 0.0299, 0.0382,\n",
      "        0.0269, 0.0225, 0.0408, 0.0221, 0.0183, 0.0240, 0.0419, 0.0353, 0.0386,\n",
      "        0.0277, 0.0337, 0.0333, 0.0258, 0.0451, 0.0115, 0.0303, 0.0236, 0.0251,\n",
      "        0.0193, 0.0470, 0.0130, 0.0254, 0.0669, 0.0474, 0.0434, 0.0175, 0.0171,\n",
      "        0.0398, 0.0770, 0.0420, 0.0497, 0.0545, 0.0367, 0.0423, 0.0349, 0.0174,\n",
      "        0.0649, 0.0482, 0.0364, 0.0530, 0.0367, 0.0162, 0.0222, 0.0357, 0.0316,\n",
      "        0.0352, 0.0784, 0.0588, 0.0205, 0.0286, 0.0190, 0.0356, 0.0438, 0.0342,\n",
      "        0.0250, 0.0123, 0.0165, 0.0510, 0.0627, 0.0582, 0.0516, 0.0480])\n",
      "Test Loss: 0.1173, Accuracy: 0.0000,\n",
      "Neg_Precision: 0.1461 = 13/89, Neg_Recall: 0.8125 = 13/16, Neg_F1: 0.1238\n",
      "correct_cnt 13, neg_pred_cnt 89\n"
     ]
    }
   ],
   "source": [
    "autoencoder.eval()  # Set the model to evaluation mode\n",
    "total_loss = 0.0\n",
    "correct = 0\n",
    "# error_correct = 0\n",
    "total = 0\n",
    "correct_cnt = 0\n",
    "neg_pred_cnt = 0\n",
    "\n",
    "threshold = 0.048\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():  # No gradients needed for evaluation\n",
    "    for x_batch, labels in dataloader_test:  # Assuming labels are included in your test loader\n",
    "        # print(x_batch.shape)\n",
    "        outputs = autoencoder(x_batch)\n",
    "        loss = criterion(outputs, x_batch)  # Calculate reconstruction loss\n",
    "        total_loss += loss.item()\n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_error = torch.mean((outputs - x_batch) ** 2, dim = 1 )\n",
    "        print(reconstruction_error)\n",
    "        # Identify anomalies based on the threshold\n",
    "        predictions = (reconstruction_error > threshold).float() \n",
    "\n",
    "        # Compare predictions with actual labels\n",
    "        # error_mask = (labels == 0)\n",
    "        # error_correct += ((predictions[error_mask] == 0).sum()).item()  \n",
    "        # correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_labels.extend(labels.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "average_loss = total_loss / len(dataloader_test)\n",
    "accuracy = correct / total\n",
    "\n",
    "TN = conf_matrix[0, 0]  \n",
    "FP = conf_matrix[0, 1] \n",
    "FN = conf_matrix[1, 0]  \n",
    "TP = conf_matrix[1, 1]  \n",
    "Neg_precision = TN/(FN+TN)\n",
    "Neg_recall =TN/(FP+TN)\n",
    "\n",
    "for i in range(len(all_labels)) :\n",
    "    if all_labels[i] == 0 and all_predictions[i] == 0:\n",
    "        correct_cnt += 1\n",
    "for pred in all_predictions:\n",
    "    if pred == 0:\n",
    "        neg_pred_cnt += 1\n",
    "\n",
    "print(f\"Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f},\")\n",
    "print(f'Neg_Precision: {Neg_precision:.4f} = {TN}/{FN + TN}, Neg_Recall: {Neg_recall:.4f} = {TN}/{FP + TN}, Neg_F1: {(Neg_precision * Neg_recall)/(Neg_precision + Neg_recall):.4f}') \n",
    "print(f'correct_cnt {correct_cnt}, neg_pred_cnt {neg_pred_cnt}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bda12-8588-4f4d-a79c-c7117a335112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfbb4d-213c-46c2-a438-e1e2f6d1132d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
